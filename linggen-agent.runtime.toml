[[models]]
id = "qwen3.5:35b"
provider = "ollama"
url = "http://127.0.0.1:11434"
model = "qwen3.5:35b"
keep_alive = "10m"
tags = []

[[models]]
id = "qwen3.5:cloud"
provider = "ollama"
url = "http://127.0.0.1:11434"
model = "qwen3.5:cloud"
keep_alive = "20m"
context_window = 131072
tags = []

[[models]]
id = "gemini3-flash"
provider = "gemini"
url = "https://generativelanguage.googleapis.com/v1beta/openai"
model = "gemini-3-flash-preview"
keep_alive = "20m"
tags = []

[[models]]
id = "gemini-2.5-flash"
provider = "gemini"
url = "https://generativelanguage.googleapis.com/v1beta/openai"
model = "gemini-2.5-flash"
tags = []

[server]
port = 9898

[agent]
max_iters = 100
write_safety_mode = "warn"
tool_permission_mode = "ask"
prompt_loop_breaker = "You are repeatedly calling '{}' with the same arguments and not making progress. Use a different tool/arguments and continue automatically."
max_delegation_depth = 2

[logging]
level = "debug"
retention_days = 7

[[agents]]
id = "lead"
spec_path = "agents/lead.md"
model = "ollama1"

[[agents]]
id = "coder"
spec_path = "agents/coder.md"
model = "ollama1"

[[agents]]
id = "search"
spec_path = "agents/search.md"
model = "ollama1"

[[agents]]
id = "plan"
spec_path = "agents/plan.md"
model = "ollama1"

[routing]
policies = []
default_models = [
    "gemini-flash",
    "qwen3-coder:latest",
    "qwen3.5:35b",
]
